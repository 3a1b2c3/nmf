diff --git a/train.py b/train.py
index ed02bac..f43464b 100644
--- a/train.py
+++ b/train.py
@@ -47,16 +47,16 @@ def render_test(args):
         logger.info('the ckpt path does not exists!!')
         return
 
-    ckpt = torch.load(args.ckpt)
-    tensorf = TensorNeRF.load(ckpt, args.model.arch, strict=False).to(device)
-    tensorf.sampler.update(tensorf.rf, init=True)
-
     # init dataset
     dataset = dataset_dict[args.dataset.dataset_name]
     test_dataset = dataset(os.path.join(args.datadir, args.dataset.scenedir), split='test', downsample=args.dataset.downsample_train, is_stack=True)
     white_bg = test_dataset.white_bg
     ndc_ray = args.dataset.ndc_ray
 
+    ckpt = torch.load(args.ckpt)
+    tensorf = TensorNeRF.load(ckpt, args.model.arch, near_far=test_dataset.near_far, strict=False).to(device)
+    tensorf.sampler.update(tensorf.rf, init=True)
+
     logfolder = os.path.dirname(args.ckpt)
     if args.render_train:
         os.makedirs(f'{logfolder}/imgs_train_all', exist_ok=True)
@@ -125,11 +125,12 @@ def reconstruction(args):
         # tensorf.rf.update_stepSize(grid_size)
 
     # TODO REMOVE
-    # bg_sd = torch.load('log/mats360_bg.th')
-    # from models import bg_modules
-    # bg_module = bg_modules.HierarchicalCubeMap(bg_resolution=2048, num_levels=1, featureC=128, activation='softplus', power=2, lr=1e-2)
-    # bg_module.load_state_dict(bg_sd, strict=False)
-    # tensorf.bg_module = bg_module
+    if args.fixed_bg:
+        bg_sd = torch.load('log/mats360_bg.th')
+        from models import bg_modules
+        bg_module = bg_modules.HierarchicalCubeMap(bg_resolution=2048, num_levels=1, featureC=128, activation='softplus', power=2, lr=1e-2)
+        bg_module.load_state_dict(bg_sd, strict=False)
+        tensorf.bg_module = bg_module
 
     tensorf = tensorf.to(device)
 
@@ -151,11 +152,6 @@ def reconstruction(args):
         ind = [i for i, d in enumerate(grad_vars) if 'name' in d and d['name'] == 'bg'][0]
         grad_vars[ind]['params'] = tensorf.bg_module.parameters()
         grad_vars[ind]['lr'] = lr_bg
-    # optimizer = torch.optim.Adam(grad_vars, betas=(0.9, 0.999), weight_decay=0, eps=1e-6)
-    optimizer = torch.optim.Adam(grad_vars, betas=(0.9, 0.99), weight_decay=0)
-    # optimizer = torch.optim.SGD(grad_vars, momentum=0.9, weight_decay=0)
-    # optimizer = torch.optim.RMSprop(grad_vars, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0)
-    # smoothing_vals = torch.linspace(0.5, 0.5, len(upsamp_list)+1).tolist()[1:]
 
 
     torch.cuda.empty_cache()
@@ -210,8 +206,10 @@ def reconstruction(args):
 
     # TODO REMOVE
     if args.ckpt is None:
+        # dparams = tensorf.parameters()
+        # space_optim = torch.optim.Adam(tensorf.rf.dbasis_mat.parameters(), lr=0.5, betas=(0.9,0.99))
         space_optim = torch.optim.Adam(tensorf.parameters(), lr=0.005, betas=(0.9,0.99))
-        pbar = tqdm(range(1000))
+        pbar = tqdm(range(tensorf.rf.num_pretrain))
         for _ in pbar:
             xyz = torch.rand(20000, 3, device=device)*2-1
             sigma_feat = tensorf.rf.compute_densityfeature(xyz)
@@ -219,7 +217,11 @@ def reconstruction(args):
             alpha = 1-torch.exp(-sigma_feat * 0.015 * tensorf.rf.distance_scale)
             # sigma = 1-torch.exp(-sigma_feat)
             # loss = (sigma-torch.rand_like(sigma)*args.start_density).abs().mean()
-            loss = (alpha-params.start_density).abs().mean()
+            # target_alpha = (params.start_density+params.start_density*(2*torch.rand_like(alpha)-1))
+            target_alpha = (params.start_density + 0.1*params.start_density*torch.randn_like(alpha))
+            # target_alpha = target_alpha.clip(min=params.start_density/2, max=params.start_density*2)
+            # target_alpha = params.start_density
+            loss = (alpha-target_alpha).abs().mean()
             # loss = (-sigma[mask].clip(max=1).sum() + sigma[~mask].clip(min=1e-8).sum())
             space_optim.zero_grad()
             loss.backward()
@@ -232,24 +234,35 @@ def reconstruction(args):
 
     pbar = tqdm(range(params.n_iters), miniters=args.progress_refresh_rate, file=sys.stdout)
     old_decay = False
-    # T_max = 30000
-    # scheduler1 = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)
-    # scheduler2 = lr_scheduler.ChainedScheduler([
-    #         lr_scheduler.ConstantLR(optimizer, factor=0.25, total_iters=600000),
-    #         lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10000, T_mult=1)
-    # ])
-    # scheduler = lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[3000])
-    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=params.n_iters, T_mult=1, eta_min=1e-3)
-    # scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1000, T_mult=1, eta_min=1e-3)
+    def init_optimizer(grad_vars):
+        # optimizer = torch.optim.Adam(grad_vars, betas=(0.9, 0.999), weight_decay=0, eps=1e-6)
+        optimizer = torch.optim.Adam(grad_vars, betas=(0.9, 0.99))
+        # optimizer = torch.optim.SGD(grad_vars, momentum=0.9, weight_decay=0)
+        # optimizer = torch.optim.RMSprop(grad_vars, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0)
+        # smoothing_vals = torch.linspace(0.5, 0.5, len(upsamp_list)+1).tolist()[1:]
+        # T_max = 30000
+        # scheduler1 = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)
+        # scheduler2 = lr_scheduler.ChainedScheduler([
+        #         lr_scheduler.ConstantLR(optimizer, factor=0.25, total_iters=600000),
+        #         lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10000, T_mult=1)
+        # ])
+        # scheduler = lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[3000])
+        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=params.n_iters, T_mult=1, eta_min=1e-3)
+        # scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1000, T_mult=1, eta_min=1e-3)
+        return optimizer, scheduler
+    optimizer, scheduler = init_optimizer(grad_vars)
     if True:
     # with torch.profiler.profile(record_shapes=True, schedule=torch.profiler.schedule(wait=1, warmup=1, active=20), with_stack=True) as p:
     # with torch.autograd.detect_anomaly():
         for iteration in pbar:
 
-            if iteration < 500:
-                ray_idx, rgb_idx = trainingSampler.nextids(batch=params.batch_size//3)
+            if iteration < 50:
+                ray_idx, rgb_idx = trainingSampler.nextids(batch=params.batch_size//8)
+            elif iteration < 500:
+                ray_idx, rgb_idx = trainingSampler.nextids(batch=params.batch_size//4)
             else:
                 ray_idx, rgb_idx = trainingSampler.nextids()
+            # ray_idx, rgb_idx = trainingSampler.nextids()
 
             # patches = allrgbs[ray_idx].reshape(-1, args.bundle_size, args.bundle_size, 3)
             # plt.imshow(patches[0])
@@ -266,7 +279,7 @@ def reconstruction(args):
             with torch.cuda.amp.autocast(enabled=args.fp16):
             # if True:
                 data = renderer(rays_train, tensorf,
-                        keys = ['rgb_map', 'floater_loss', 'normal_loss', 'backwards_rays_loss', 'diffuse_reg', 'bounce_count', 'color_count', 'roughness', 'whole_valid'],
+                        keys = ['rgb_map', 'floater_loss', 'normal_loss', 'backwards_rays_loss', 'diffuse_reg', 'bounce_count', 'color_count', 'roughness', 'whole_valid', 'normal_map'],
                         focal=focal, output_alpha=alpha_train, chunk=params.batch_size, white_bg = white_bg, is_train=True, ndc_ray=ndc_ray)
 
                 # loss = torch.mean((rgb_map[:, 1, 1] - rgb_train[:, 1, 1]) ** 2)
@@ -281,28 +294,35 @@ def reconstruction(args):
                     loss = torch.sqrt((rgb_map - rgb_train[whole_valid]) ** 2 + params.charbonier_eps**2).mean()
                 else:
                     # loss = ((rgb_map - rgb_train[whole_valid]) ** 2).mean()
-                    loss = F.huber_loss(rgb_map, rgb_train[whole_valid], delta=1, reduction='mean')
+                    # loss = F.huber_loss(rgb_map.clip(0, 1), rgb_train[whole_valid], delta=1, reduction='mean')
+                    loss = ((rgb_map.clip(0, 1) - rgb_train[whole_valid].clip(0, 1))**2).mean()
+                # gt_normal_map = test_dataset.all_norms[ray_idx].to(device)
+                # norm_err = -(data['normal_map'] * gt_normal_map).sum(dim=-1).mean()
+                # ic(norm_err)
                 # loss = torch.sqrt(F.huber_loss(rgb_map, rgb_train, delta=1, reduction='none') + params.charbonier_eps**2).mean()
-                photo_loss = ((rgb_map.clip(0, 1) - rgb_train[whole_valid].clip(0, 1)) ** 2).mean().detach()
+                # photo_loss = ((rgb_map.clip(0, 1) - rgb_train[whole_valid].clip(0, 1)) ** 2).mean().detach()
+                photo_loss = loss.detach()
                 backwards_rays_loss = data['backwards_rays_loss']
 
                 # loss
                 total_loss = loss + \
-                    params.normal_lambda*normal_loss + \
                     params.floater_lambda*floater_loss + \
                     params.backwards_rays_lambda*backwards_rays_loss + \
-                    params.diffuse_lambda * diffuse_reg
+                    params.diffuse_lambda * diffuse_reg# + \
+                    # 1.0 * norm_err
                 # ic(total_loss, params.normal_lambda*normal_loss, params.floater_lambda*floater_loss, params.backwards_rays_lambda*backwards_rays_loss, params.diffuse_lambda*diffuse_reg)
+                if iteration > 0000:
+                    total_loss += params.normal_lambda*normal_loss
 
                 if tensorf.visibility_module is not None:
-                    if iteration % 10 == 0:
-                        if iteration < 100:# or iteration % 1000 == 0:
-                            visibility_loss = tensorf.init_vis_module()
+                    pass
+                    if iteration % 1 == 0 and iteration > 1000:
+                        # if iteration < 100 or iteration % 1000 == 0:
+                        if iteration % 500 == 0 and iteration < 5000:
+                            tensorf.init_vis_module()
+                            torch.cuda.empty_cache()
                         else:
-                            visibility_loss = tensorf.compute_visibility_loss(params.N_visibility_rays)
-                    total_loss += params.visibility_lambda * visibility_loss
-                else:
-                    visibility_loss = 0
+                            tensorf.compute_visibility_loss(params.N_visibility_rays)
 
                 if ortho_reg_weight > 0:
                     loss_reg = tensorf.rf.vector_comp_diffs()
@@ -344,9 +364,6 @@ def reconstruction(args):
             summary_writer.add_scalar('train/color_count', data['color_count'].sum(), global_step=iteration)
             summary_writer.add_scalar('train/bounce_count', data['bounce_count'], global_step=iteration)
 
-            if old_decay:
-                for param_group in optimizer.param_groups:
-                    param_group['lr'] = param_group['lr'] * lr_factor
             summary_writer.add_scalar('train/lr', list(optimizer.param_groups)[0]['lr'], global_step=iteration)
 
             # logger.info the current values of the losses.
@@ -354,12 +371,12 @@ def reconstruction(args):
                 pbar.set_description(
                     f'psnr = {float(np.mean(PSNRs)):.2f}'
                     + f' test_psnr = {float(np.mean(PSNRs_test)):.2f}'
+                    + f' loss = {total_loss.detach().item():.5f}'
                     + f' rough = {data["roughness"].mean().item():.5f}'
                     + f' nerr = {float(normal_loss):.1e}'
                     + f' back = {backwards_rays_loss:.5e}'
                     + f' float = {floater_loss:.1e}'
-                    + f' mipbias = {float(tensorf.bg_module.mipbias):.1e}'
-                    + f' vis = {float(visibility_loss):.1e}'
+                    # + f' mipbias = {float(tensorf.bg_module.mipbias):.1e}'
                     # + f' mse = {photo_loss:.6f}'
                 )
                 PSNRs = []
@@ -378,9 +395,11 @@ def reconstruction(args):
                     tensorf.save(f'{logfolder}/{args.expname}_{iteration:06d}.th', args.model.arch)
 
             if tensorf.check_schedule(iteration):
-                new_grad_vars = tensorf.get_optparam_groups()
-                for param_group, new_param_group in zip(optimizer.param_groups, new_grad_vars):
-                    param_group['params'] = new_param_group['params']
+                grad_vars = tensorf.get_optparam_groups()
+                optimizer, scheduler = init_optimizer(grad_vars)
+                # new_grad_vars = tensorf.get_optparam_groups()
+                # for param_group, new_param_group in zip(optimizer.param_groups, new_grad_vars):
+                #     param_group['params'] = new_param_group['params']
 
             # if iteration in update_alphamask_list:
 

